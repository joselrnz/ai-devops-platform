# Tempo distributed values for production tracing

tempo:
  structuredConfig:
    # Multitenancy
    multitenancy_enabled: false

    # Distributor
    distributor:
      receivers:
        jaeger:
          protocols:
            grpc:
              endpoint: 0.0.0.0:14250
            thrift_http:
              endpoint: 0.0.0.0:14268
            thrift_binary:
              endpoint: 0.0.0.0:6832
            thrift_compact:
              endpoint: 0.0.0.0:6831
        zipkin:
          endpoint: 0.0.0.0:9411
        otlp:
          protocols:
            http:
              endpoint: 0.0.0.0:4318
            grpc:
              endpoint: 0.0.0.0:4317

    # Ingester
    ingester:
      trace_idle_period: 10s
      max_block_bytes: 1000000
      max_block_duration: 5m

    # Compactor
    compactor:
      compaction:
        block_retention: 168h  # 7 days

    # Storage
    storage:
      trace:
        backend: s3
        s3:
          bucket: tempo-traces
          endpoint: s3.us-east-1.amazonaws.com
          region: us-east-1
        wal:
          path: /var/tempo/wal
        pool:
          max_workers: 100
          queue_depth: 10000

    # Query Frontend
    query_frontend:
      search:
        duration_slo: 5s
        throughput_bytes_slo: 1.073741824e+09
      trace_by_id:
        duration_slo: 5s

    # Overrides
    overrides:
      defaults:
        metrics_generator:
          processors: [service-graphs, span-metrics]
          generate_native_histograms: true

# Gateway
gateway:
  enabled: true
  replicas: 2
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 256Mi
  ingress:
    enabled: true
    ingressClassName: nginx
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt-prod
    hosts:
      - host: tempo.example.com
        paths:
          - path: /
            pathType: Prefix
    tls:
      - secretName: tempo-tls
        hosts:
          - tempo.example.com

# Distributor
distributor:
  replicas: 2
  resources:
    requests:
      cpu: 500m
      memory: 1Gi
    limits:
      cpu: 1000m
      memory: 2Gi
  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80

# Ingester
ingester:
  replicas: 3
  resources:
    requests:
      cpu: 500m
      memory: 2Gi
    limits:
      cpu: 1000m
      memory: 4Gi
  persistence:
    enabled: true
    size: 50Gi
    storageClass: gp3

# Querier
querier:
  replicas: 2
  resources:
    requests:
      cpu: 500m
      memory: 1Gi
    limits:
      cpu: 1000m
      memory: 2Gi
  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 6
    targetCPUUtilizationPercentage: 70

# Query Frontend
queryFrontend:
  replicas: 2
  resources:
    requests:
      cpu: 250m
      memory: 512Mi
    limits:
      cpu: 500m
      memory: 1Gi
  query:
    enabled: true
    resources:
      requests:
        cpu: 250m
        memory: 256Mi
      limits:
        cpu: 500m
        memory: 512Mi

# Compactor
compactor:
  replicas: 1
  resources:
    requests:
      cpu: 250m
      memory: 512Mi
    limits:
      cpu: 500m
      memory: 1Gi

# Metrics Generator
metricsGenerator:
  enabled: true
  replicas: 1
  resources:
    requests:
      cpu: 250m
      memory: 512Mi
    limits:
      cpu: 500m
      memory: 1Gi
  config:
    processor:
      service_graphs:
        dimensions:
          - service
          - cluster
          - namespace
      span_metrics:
        dimensions:
          - service
          - cluster
          - namespace
          - http.method
          - http.status_code
    storage:
      path: /var/tempo/generator/wal
      remote_write:
        - url: http://kube-prometheus-stack-prometheus:9090/api/v1/write
          send_exemplars: true

# Service Monitor
serviceMonitor:
  enabled: true
  interval: 30s
  labels:
    release: kube-prometheus

# Memberlist (for ring)
memcached:
  enabled: false  # Using memberlist instead

# Traces to metrics
traces_to_metrics:
  enabled: true
